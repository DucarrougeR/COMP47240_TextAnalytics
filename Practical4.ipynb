{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Romain Ducarrouge\n",
    "## Practical 4\n",
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "# Get our required items for GOOGLE Places API calls\n",
    "Base = \"https://maps.googleapis.com/maps/api/place/details/json?placeid=\"\n",
    "API_Key = \"__YOUR_API_KEY__\"\n",
    "Place_ID1 = \"ChIJKxDbe_lYwokRVf__s8CPn-o\"   # for Museum of Modern Art, New York\n",
    "Place_ID2 = \"ChIJgVtmM_ZYwokRcJJnebe5KX8\"   # for Museum of Arts and Design, New York\n",
    "\n",
    "# Formatting the final calls for the two pastry shops in Paris\n",
    "API_Call1 = Base+Place_ID1+\"&language=english&output=JSON&key=\"+API_Key\n",
    "API_Call2 = Base+Place_ID2+\"&language=english&output=JSON&key=\"+API_Key\n",
    "\n",
    "data1 = urlopen(API_Call1).read().decode('utf8')\n",
    "data2 = urlopen(API_Call2).read().decode('utf8')\n",
    "obj = json.loads(data1)\n",
    "obj2 = json.loads(data2)\n",
    "\n",
    "reviews = []\n",
    "for i in range (0,5): reviews.append(obj['result']['reviews'][i]['text']), reviews.append(obj2['result']['reviews'][i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "reviews = ''.join(reviews).lower()            # normalize text, lower\n",
    "reviews = re.sub(r'\\d+', '', reviews)         # remove numbers\n",
    "\n",
    "stop_words = set(stopwords.words('english'))  # remove stop words\n",
    "word_tokens = word_tokenize(reviews)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "reviews2 =[]\n",
    "for elem in filtered_sentence:     # remove punctuation\n",
    "    if elem.isalpha(): \n",
    "        reviews2.append(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('art', 9), ('museum', 6), ('place', 5), ('one', 5), ('well', 4), ('around', 4), ('see', 4), ('world', 3), ('modern', 3), ('backpack', 3), ('moma', 3), ('walk', 3), ('even', 3), ('amazing', 3), ('visit', 3), ('sure', 3), ('th', 3), ('nice', 3), ('hours', 2), ('night', 2)]\n"
     ]
    }
   ],
   "source": [
    "import math, collections\n",
    "\n",
    "count = collections.Counter(reviews2)\n",
    "print(count.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud for words with more than 2 occurences in corpus:\n",
    "<img src=\"Q1_P2_Cloud.png\",width=350,height=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the top 10 PMI pair results are:  [('advocate', 'emerging'), ('aileen', 'osborn'), ('although', 'impressionists'), ('andy', 'warhol'), ('anyone', 'interested'), ('anything', 'would'), ('arcitectually', 'building'), ('aside', 'buy'), ('awesome', 'expected'), ('bag', 'check')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bgm = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(reviews2)\n",
    "\n",
    "PMIscore = finder.nbest(bigram_measures.pmi, 10)\n",
    "print('the top 10 PMI pair results are: ', PMIscore)\n",
    "\n",
    "ChiSquare = finder.nbest(bigram_measures.chi_sq, 10)\n",
    "Mi_Like = finder.nbest(bigram_measures.mi_like, 10)\n",
    "# scored = finder.score_ngrams(bgm.likelihood_ratio)\n",
    "# print('\\nThe top 10 results for the Likelihood Ratio are: ', scored[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "The results above make sense because it seems like the words are related.\n",
    "\n",
    "* the words *anyone* and *interested* also make sense as they represent a subject and a verb (**anyone interested**)\n",
    "\n",
    "* the words *aileen* and *osborn* also make sense as they are the first and last name of a very famous artis (**Aileen Osborn**, also **Andy Warhol**)\n",
    "\n",
    "Finally, it is worth noticing the fact that the words *th* and *th* are also forming a bi-gram with a high score. This here can be explained by a missing step in the normalization steps of the corpus. Indeed we removed the numbers from the text, but several reviews talked about the **19th or 20th century** so by removing the numbers (\"19\" and \"20\") and the stopwords (\"or\") we are left with **th** and **th**\n",
    "\n",
    "\n",
    "--------\n",
    "\n",
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "consumer_key = \"__your_consumer_key__\"\n",
    "consumer_secret = \"__your_consumer_secret__\"\n",
    "access_token = \"__your_access_token__\"\n",
    "access_secret = \"__your_access_secret__\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "randomSet = []\n",
    "public_tweets = api.user_timeline(screen_name='RDucarrouge')\n",
    "for tweet in public_tweets:\n",
    "    randomSet.append(tweet.text)\n",
    "randomSet = randomSet[-10:]  # keep only the 10 most recent tweets\n",
    "\n",
    "spamSet  = ['Have you heard millions of people are making $5k+/Mo from home? heres how...t.co/blah',\n",
    "            'Was this blog you posted really necessary? tniX.biz/ad08 some kind of joke?',\n",
    "            'Someone said this real bad thing about you in a blog.... tniXurl.com/uyT5z0',\n",
    "            'I just found out who viewed me on twitter! find whos watching yours ti.co/blah',\n",
    "            'FIND OUT WHO STALKS YOUR TWITTER! THIS NEW APP ROCKS! ti.co/blah',\n",
    "            'viagra,cialis,soma,tramadol and more. no prescription. ti.co/bbvrp0l',\n",
    "            'Gain over 1,000 followers a week by using: ti.co/blah',\n",
    "            \"his is NUTS, I swear I'm getting 100s of followers from this program!! Check it t.c/blah\",\n",
    "            \"I lost a bunch of weight, this stuff works! look at news article cXXnewz.com\",\n",
    "            \"Somone said this real bad thing about you in a blog.... tniXurl.com/blah\"]\n",
    "# source: http://dianatrees.blogspot.ie/2011/07/examples-of-twitter-spam-and-malware.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Entropy for the Random Set is:\t 4.705171476497202\n",
      "-----------------------------------------------------------\n",
      "The Entropy for the Spam Set is:\t 4.355263987716375\n",
      "-----------------------------------------------------------\n",
      "The Entropy for the Entire Set is:\t 3.321928094887362\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def entropy(labels):\n",
    "    freqdist = nltk.FreqDist(labels)\n",
    "    probs = [freqdist.freq(l) for l in freqdist]\n",
    "    return -sum(p * math.log(p,2) for p in probs)\n",
    "\n",
    "randomEntropy = entropy(randomSet)\n",
    "spamEntropy = entropy(spamSet)\n",
    "\n",
    "entropylist = 0\n",
    "for elem in randomSet: entropylist += entropy(elem)\n",
    "print('The Entropy for the Random Set is:\\t', entropylist/len(randomSet))\n",
    "print('-----------------------------------------------------------')\n",
    "\n",
    "entropylist2 = 0\n",
    "for elem in spamSet: entropylist2 += entropy(elem)\n",
    "print('The Entropy for the Spam Set is:\\t', entropylist2/len(spamSet))\n",
    "print('-----------------------------------------------------------')\n",
    "\n",
    "entireSet = []\n",
    "entireSet.append(randomSet)\n",
    "entireSet.append(spamSet)\n",
    "\n",
    "entropylist3 = 0\n",
    "for elem in entireSet: entropylist3 += entropy(elem)\n",
    "print('The Entropy for the Entire Set is:\\t', entropylist3/len(entireSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @CodeBunk: Six years of Go\n",
      "\n",
      "https://t.co/VMdh2r9CRc\n",
      "\n",
      "#programming #coding #golang \n",
      "\n",
      "@ammarrehman ehazukebup https://t.co/vemkvFEsFl \n",
      "\n",
      "RT @eastdakota: There's an interesting PhD thesis to be written in how Microsoft's Tay AI bot fiasco is a case study on how real people bec… \n",
      "\n",
      "RT @CodeBunk: Using Graph Theory to Build a Recommendation Engine in JavaScript\n",
      "https://t.co/JgMDckwGGF\n",
      "#programming #coding  #javascript #… \n",
      "\n",
      "The Everyday Carry Giveaway | Creative Bloq Deals https://t.co/Ghr1ZxzKta via @creativebloq \n",
      "\n",
      "RT @ForbesTech: In this high-tech apartment block, volunteers will live like human lab rats: https://t.co/LHrom51q0N https://t.co/PcFMLbaFke \n",
      "\n",
      "RT @BV: A high-speed trading firm deleted some code by accident, explains @matt_levine. https://t.co/yliCWlDX3v https://t.co/ZT9QhAeySv \n",
      "\n",
      "RT @TechCrunch: Do you have to be in San Francisco to be a tech entrepreneur? http://t.co/Jpj49WPbpT http://t.co/l0wohaeCbf \n",
      "\n",
      "Small and medium companies Innovation Contest, win $10,000!!! http://t.co/RtJOFKu0\n",
      "@EntMagazine @LetsStartup_SF @RocketSpace \n",
      "\n",
      "RT @BloombergNews: Apple iPhone 5 may debut in October | http://t.co/vYlaY4dY \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for elem in randomSet: print(elem,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have you heard millions of people are making $5k+/Mo from home? heres how...t.co/blah \n",
      "\n",
      "Was this blog you posted really necessary? tniX.biz/ad08 some kind of joke? \n",
      "\n",
      "Someone said this real bad thing about you in a blog.... tniXurl.com/uyT5z0 \n",
      "\n",
      "I just found out who viewed me on twitter! find whos watching yours ti.co/blah \n",
      "\n",
      "FIND OUT WHO STALKS YOUR TWITTER! THIS NEW APP ROCKS! ti.co/blah \n",
      "\n",
      "viagra,cialis,soma,tramadol and more. no prescription. ti.co/bbvrp0l \n",
      "\n",
      "Gain over 1,000 followers a week by using: ti.co/blah \n",
      "\n",
      "his is NUTS, I swear I'm getting 100s of followers from this program!! Check it t.c/blah \n",
      "\n",
      "I lost a bunch of weight, this stuff works! look at news article cXXnewz.com \n",
      "\n",
      "Somone said this real bad thing about you in a blog.... tniXurl.com/blah \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for elem in spamSet: print(elem,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
